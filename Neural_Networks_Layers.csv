Capa,Propósito,Entrada,Salida,Aprendizaje
Dense (Completamente Conectada),Aprender combinaciones no lineales complejas.,Vector,Vector,Sí
Pooling (Max/Average),Reducir dimensionalidad espacial manteniendo información clave.,Mapa de características,Mapa de características reducido,No
Convolution (Convolución),Extraer características locales mediante filtros.,Mapa de características,Mapa de características,Sí
Batch Normalization,Normalizar activaciones para estabilizar y acelerar el entrenamiento.,Cualquier tensor intermedio,Tensor normalizado,"Sí (parcial, aprende parámetros de escalado y sesgo)"
Dropout,Regularizar apagando aleatoriamente neuronas.,Vector o mapa de características,Vector o mapa reducido,No
Recurrent (RNN/LSTM/GRU),Procesar datos secuenciales y capturar dependencias temporales.,Secuencia (serie temporal o texto),Secuencia o vector,Sí
Transformer,Capturar relaciones globales en secuencias mediante atención.,Secuencia,Secuencia o vector,Sí
Embedding,Convertir datos categóricos en representaciones densas.,Categoría (índices),Vector denso,Sí
Flatten,Convertir mapas de características en vectores planos.,Mapa de características,Vector,No
Activation (ReLU/Sigmoid/Tanh),Introducir no linealidad al modelo.,Vector o tensor,Vector o tensor transformado,No
Residual,Ayudar a entrenar redes profundas al permitir conexiones residuales.,Tensor,Tensor,Sí
Upsampling (Transposed Convolution),Aumentar la dimensionalidad espacial.,Mapa de características,Mapa de características ampliado,Sí
"Attention (Self-Attention, Multihead)",Focalizarse en partes relevantes de los datos.,Tensor,Tensor transformado,Sí
Normalization (Layer/Instance/Group),Normalizar activaciones a diferentes niveles.,Tensor,Tensor normalizado,"Sí (parcial, aprende parámetros de escalado y sesgo)"
